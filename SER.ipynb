{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaybeberry9591/advance_machine_learning/blob/main/SER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7D3IHrJ4hpGq"
      },
      "outputs": [],
      "source": [
        "#RAVDESS: Ryerson Audio-Visual Database of Emotional Speech and Song\n",
        "#Data Preprocessing - Importing the Libraries\n",
        "\n",
        "import librosa    #Python Library for analysing audio\n",
        "import soundfile\n",
        "import os, glob, pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pylab as plt\n",
        "import seaborn as sns\n",
        "import librosa.display\n",
        "import IPython.display as ipd\n",
        "from itertools import cycle\n",
        "\n",
        "sns.set_theme(style=\"white\", palette=None)\n",
        "color_pal = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
        "color_cycle = cycle(plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"])\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neural_network import MLPClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfNNT6A5utQo"
      },
      "outputs": [],
      "source": [
        "def extract_features(X, sample_rate, mfcc, chroma, mel):\n",
        "    result = np.array([])\n",
        "\n",
        "    if chroma:\n",
        "        stft = np.abs(librosa.stft(X))\n",
        "\n",
        "    if mfcc:\n",
        "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
        "        result = np.hstack((result, mfccs))\n",
        "\n",
        "    if chroma:\n",
        "        chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
        "        result = np.hstack((result, chroma))\n",
        "\n",
        "    if mel:\n",
        "        mel = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T, axis=0)\n",
        "        result = np.hstack((result, mel))\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysbg0epahpGt"
      },
      "outputs": [],
      "source": [
        "#Emotions in the RAVDESS dataset\n",
        "\n",
        "emotions = {\n",
        "  '01':'neutral',\n",
        "  '02':'calm',\n",
        "  '03':'happy',\n",
        "  '04':'sad',\n",
        "  '05':'angry',\n",
        "  '06':'fearful',\n",
        "  '07':'disgust',\n",
        "  '08':'surprised'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1PW6TY6hpGu"
      },
      "outputs": [],
      "source": [
        "#Emotions to observe\n",
        "\n",
        "observed_emotions=['neutral','happy','angry']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oqlp8vBkh4sq",
        "outputId": "3707798f-820c-4877-9f93-9b61a9d27a8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpnQ9VvHhpGu"
      },
      "outputs": [],
      "source": [
        "#importing the data and extracting features for each sound file\n",
        "def load_data(test_size=0.20):\n",
        "    X, y = [], []\n",
        "    for file in glob.glob(\"/content/drive/MyDrive/Dataset/Dataset/*/*.wav\"):\n",
        "        file_name = os.path.basename(file)\n",
        "        emotion = emotions[file_name.split(\"-\")[2]]\n",
        "        if emotion not in observed_emotions:\n",
        "            continue\n",
        "        audio_data, sample_rate = librosa.load(file, sr=None, dtype=np.float32)\n",
        "        feature = extract_features(audio_data, sample_rate, mfcc=True, chroma=True, mel=True)\n",
        "        X.append(feature)\n",
        "        y.append(emotion)\n",
        "    return train_test_split(np.array(X), y, test_size = test_size, train_size=0.75, random_state = 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgPgFHi5hpGu"
      },
      "outputs": [],
      "source": [
        "#Splitting the dataset\n",
        "X_train, X_test, y_train, y_test = load_data(test_size = 0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWqkbm8zhpGu",
        "outputId": "3246f6f3-69b8-4e1d-e6ab-88c4f23f63b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(360, 120)\n"
          ]
        }
      ],
      "source": [
        "#Getting the shape of the training and testing datasets\n",
        "\n",
        "print((X_train.shape[0], X_test.shape[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMEn06bphpGv",
        "outputId": "57b45d84-70b0-4534-c1fe-bb2c25449b7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features extracted: 180\n"
          ]
        }
      ],
      "source": [
        "#Getting the number of features extracted\n",
        "\n",
        "print(f'Features extracted: {X_train.shape[1]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AG87Ezm3hpGv"
      },
      "outputs": [],
      "source": [
        "#Initializing the Multi Layer Perceptron Classifier\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "classifier = MLPClassifier(alpha = 0.01, batch_size = 256, epsilon = 1e-08, hidden_layer_sizes = (300,),\n",
        "                    learning_rate='adaptive', max_iter = 500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "Ta3ZU0GthpGv",
        "outputId": "fc85602b-2bd5-4d66-d8f3-d525f11cc1ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(alpha=0.01, batch_size=256, hidden_layer_sizes=(300,),\n",
              "              learning_rate='adaptive', max_iter=500)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(alpha=0.01, batch_size=256, hidden_layer_sizes=(300,),\n",
              "              learning_rate=&#x27;adaptive&#x27;, max_iter=500)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(alpha=0.01, batch_size=256, hidden_layer_sizes=(300,),\n",
              "              learning_rate=&#x27;adaptive&#x27;, max_iter=500)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "#Training the model\n",
        "\n",
        "classifier.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZRtgGRBhpGw"
      },
      "outputs": [],
      "source": [
        "#Prediction for the test set\n",
        "\n",
        "y_pred = classifier.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dx0siyuYhpGw"
      },
      "outputs": [],
      "source": [
        "#Calculating the accuracy of our model\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(y_true = y_test, y_pred = y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4EW3_a0hpGw",
        "outputId": "7c7e5e3a-f423-4c96-d234-8763234a3993"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 84.17%\n"
          ]
        }
      ],
      "source": [
        "#Printing the accuracy\n",
        "\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6YpPw8mhpGy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc47d6db-5412-4864-c7cf-e5f38e28dfdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted emotion: 01\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Load the new audio file and extract features\n",
        "def predict_emotion(file_path):\n",
        "    audio_data, sample_rate = librosa.load(file_path, sr=None, dtype=np.float32)\n",
        "    new_data_features = extract_features(audio_data, sample_rate, mfcc=True, chroma=True, mel=True)\n",
        "    return new_data_features\n",
        "\n",
        "# Step 2: Use the trained classifier to predict the emotion\n",
        "def predict_with_classifier(classifier, features):\n",
        "    return classifier.predict([features])\n",
        "\n",
        "# Example of predicting a new audio file\n",
        "new_file_path = \"/content/drive/MyDrive/Dataset/Dataset/Actor_20/03-01-01-01-01-01-20.wav\"\n",
        "new_data_features = predict_emotion(new_file_path)\n",
        "\n",
        "# Use the trained classifier to predict the emotion\n",
        "predicted_label = predict_with_classifier(classifier, new_data_features)\n",
        "\n",
        "# Map the predicted label to the corresponding emotion\n",
        "emotions_inverse = {v: k for k, v in emotions.items()}  # Reverse the emotions dictionary\n",
        "predicted_emotion = emotions_inverse[predicted_label[0]]\n",
        "\n",
        "print(\"Predicted emotion:\", predicted_emotion)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}